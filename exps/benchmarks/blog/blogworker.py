# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------
# Copyright (C) 2012 by Brown University
#
# Permission is hereby granted, free of charge, to any person obtaining
# a copy of this software and associated documentation files (the
# "Software"), to deal in the Software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish,
# distribute, sublicense, and/or sell copies of the Software, and to
# permit persons to whom the Software is furnished to do so, subject to
# the following conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT
# IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
# -----------------------------------------------------------------------

import sys
import os
import string
import re
import logging
import traceback
import pymongo
from pprint import pprint, pformat

import constants
from util import *
from api.abstractworker import AbstractWorker
from api.message import *

LOG = logging.getLogger(__name__)

# BLOGWORKER
# Andy Pavlo - http://www.cs.brown.edu/~pavlo/
# 
# This is the worker for the 'blog' microbenchmark in the paper
# There are three types of experiments that we want to perform on the 
# data generated by this code. These experiments are designed to higlight
# different aspects of database design in MongoDB by demonstrated the
# performance trade-offs of getting it wrong.
# For each experiment type, there are two variations of the workload. The first 
# of which is the "correct" design choice and the second is the "bad" design
# choice. Yes, this is somewhat a simplistic view, but it's mostly 
# meant to be an demonstration rather than a deep analysis of the issues:
#
# Experiment #1: SHARDING KEYS
# For this experiment, we will shard articles by their autoinc id and then 
# by their id+timestamp. This will show that sharding on just the id won't
# work because of skew, but by adding the timestamp the documents are spread out
# more evenly.
# 
# Experiment #2: DENORMALIZATION
# In our microbenchmark we should have a collection of articles and collection of 
# article comments. The target workload will be to grab an article and grab the 
# top 10 comments for that article sorted by a user rating. In the first experiment,
# we will store the articles and comments in separate collections.
# In the second experiment, we'll embedded the comments inside of the articles.
# 
# Experiment #3: INDEXES
# In our final benchmark, we compared the performance difference between a query on 
# a collection with (1) no index for the query's predicate, (2) an index with only one 
# key from the query's predicate, and (3) a covering index that has all of the keys 
# referenced by that query.
# 
class BlogWorker(AbstractWorker):
    
    def initImpl(self, config):
        # Total number of articles in database
        self.num_articles = int(config['default']["scalefactor"] * constants.NUM_ARTICLES)
        
        # Zipfian distribution on the number of comments & their ratings
        self.commentsZipf = ZipfGenerator(constants.MAX_NUM_COMMENTS, 1.0)
        self.ratingZipf = ZipfGenerator(constants.MAX_COMMENT_RATING, 1.0)
        self.db = self.conn[constants.DB_NAME]
        
        if config['default']["reset"]:
            LOG.info("Resetting database '%s'" % constants.DB_NAME)
            self.conn.drop_database(constants.DB_NAME)
        
        # Always drop the indexes
        self.db[constants.ARTICLE_COLL].drop_indexes()
        self.db[constants.COMMENT_COLL].drop_indexes()
        
        # Sharding Key
        if config[self.name]["experiment"] == constants.EXP_SHARDING:
            self.articleZipf = ZipfGenerator(self.num_articles, 1.0)
            self.db[constants.ARTICLE_COLL].create_index([("id", pymongo.ASCENDING)])
            
            # Initialize sharding configuration
            self.initSharding(config)
            
        # Denormalization
        elif config[self.name]["experiment"] == constants.EXP_DENORMALIZATION:
            # We need an index on ARTICLE
            self.db[constants.ARTICLE_COLL].create_index([("id", pymongo.ASCENDING)])
            # And if we're not denormalized, on COMMENTS as well
            if not config[self.name]["denormalize"]:
                self.db[constants.COMMENT_COLL].create_index([("article", pymongo.ASCENDING)])
                
        # Indexing
        elif config[self.name]["experiment"] == constants.EXP_INDEXING:
            self.initIndexes(config)
            
        # Busted!
        else:
            raise Exception("Unexpected experiment type %d" % config["experiment"])
        
    ## DEF
    
    def initSharding(self, config):
        assert self.db != None
        
        # Enable sharding on the entire database
        try:
            result = self.db.command({"enablesharding": self.db.name})
            assert result["ok"] == 1, "DB Result: %s" % pformat(result)
        except:
            LOG.error("Failed to enable sharding on database '%s'" % self.db.name)
            raise
        
        # Generate sharding key patterns
        # CollectionName -> Pattern
        # http://www.mongodb.org/display/DOCS/Configuring+Sharding#ConfiguringSharding-ShardingaCollection
        shardingPatterns = { }
        
        if config[self.name]["sharding"] == constants.SHARDEXP_SINGLE:
            pass
        
        elif config[self.name]["sharding"] == constants.SHARDEXP_COMPOUND:
            pass
        
        else:
            raise Exception("Unexpected sharding configuration type '%d'" % config["sharding"])
        
        # Then enable sharding on each of these collections
        for col,pattern in shardingPatterns.iteritems():
            LOG.debug("Sharding Collection %s.%s: %s" % (self.db.name, col, pattern))
            try:
                result = self.db.command({"shardcollection": col, "key": pattern})
                assert result["ok"] == 1, "DB Result: %s" % pformat(result)
            except:
                LOG.error("Failed to enable sharding on collection '%s.%s'" % (self.db.name, col))
                raise
        ## FOR
        
        LOG.debug("Successfully enabled sharding on %d collections in database %s" % \
                  (len(shardingPatterns, self.db.name)))
    ## DEF
    
    def initIndexes(self, config):
        assert self.db != None
        
        # Nothing
        if config[self.name]["indexes"] == constants.INDEXEXP_NONE:
            pass
        # Regular Index
        elif config[self.name]["indexes"] == constants.INDEXEXP_PREDICATE:
            self.db[constants.ARTICLE_COLL].create_index([("article", pymongo.ASCENDING)])
        # Cover Index
        elif config[self.name]["indexes"] == constants.INDEXEXP_COVERING:
            self.db[constants.ARTICLE_COLL].create_index([("id", pymongo.ASCENDING), \
                                                          ("date", pymongo.ASCENDING), \
                                                          ("author", pymongo.ASCENDING)])
        # Busted!
        else:
            raise Exception("Unexpected indexing configuration type '%d'" % config["indexes"])
    ## DEF
    
    def loadImpl(self, config, channel, msg):
        assert self.conn != None
        
        # The message we're given is a tuple that contains
        # the first and articleIds that we're going to insert, and
        # the list of author names that we'll generate articles from
        firstArticle, lastArticle, authors = msg.data
        
        LOG.info("Generating %s data" % self.getBenchmarkName())
        articleCtr = 0
        commentCtr = 0
        commentId = self.getWorkerId() * 1000000
        
        ## ----------------------------------------------
        ## LOAD ARTICLES
        ## ----------------------------------------------
        articlesBatch = [ ]
        commentsBatch = [ ]
        for articleId in xrange(firstArticle, lastArticle):
            titleSize = int(random.gauss(constants.MAX_TITLE_SIZE/2, constants.MAX_TITLE_SIZE/4))
            contentSize = int(random.gauss(constants.MAX_CONTENT_SIZE/2, constants.MAX_CONTENT_SIZE/4))
            
            title = randomString(titleSize)
            slug = list(title.replace(" ", ""))
            if len(slug) > 64: slug = slug[:64]
            for idx in xrange(0, len(slug)):
                if random.randint(0, 10) == 0:
                    slug[idx] = "-"
            ## FOR
            slug = "".join(slug)
            articleDate = randomDate(constants.START_DATE, constants.STOP_DATE)
            
            article = {
                "id": articleId,
                "title": title,
                "date": articleDate,
                "author": random.choice(authors),
                "slug": slug,
                "content": randomString(contentSize)
            }

            ## ----------------------------------------------
            ## LOAD COMMENTS
            ## ----------------------------------------------
            numComments = self.commentsZipf.next()
            lastDate = articleDate
            for ii in xrange(0, numComments):
                lastDate = randomDate(lastDate, constants.STOP_DATE)
                commentAuthor = randomString(int(random.gauss(constants.MAX_AUTHOR_SIZE/2, constants.MAX_AUTHOR_SIZE/4)))
                commentContent = randomString(int(random.gauss(constants.MAX_COMMENT_SIZE/2, constants.MAX_COMMENT_SIZE/4)))
                
                comment = {
                    "id": commentId,
                    "article": articleId,
                    "date": lastDate, 
                    "author": commentAuthor,
                    "comment": commentContent,
                    "rating": int(self.ratingZipf.next())
                }
                commentCtr += 1
                if not config[self.name]["denormalize"]:
                    commentsBatch.append(comment)
                else:
                    if not "comments" in article:
                        article["comments"] = [ ]
                    article["comments"].append(comment)
                commentId += 1
            ## FOR (comments)

            # Always insert the article
            articlesBatch.append(article)
            articleCtr += 1
            if self.debug and articleCtr % 100 == 0 :
                LOG.debug("ARTICLE: %6d / %d" % (articleCtr, (lastArticle - firstArticle)))
                self.db[constants.ARTICLE_COLL].insert(articlesBatch)
                if len(commentsBatch) > 0:
                    self.db[constants.COMMENT_COLL].insert(commentsBatch)
                articlesBatch = [ ]
                commentsBatch = [ ]
        ## FOR (articles)
        if len(articlesBatch) > 0:
            self.db[constants.ARTICLE_COLL].insert(articlesBatch)
            self.db[constants.COMMENT_COLL].insert(commentsBatch)
        
        LOG.info("# of ARTICLES: %d" % articleCtr)
        LOG.info("# of COMMENTS: %d" % commentCtr)
    ## DEF
    
    def next(self, config):
        assert "experiment" in config
        
        # It doesn't matter what we pick, so we'll just 
        # return the name of the experiment
        txnName = "exp%02d" % config[self.name]["experiment"]
        params = None
        
        # Sharding Key
        if config[self.name]["experiment"] == constants.EXP_SHARDING:
            assert self.articleZipf
            params = [ int(self.articleZipf.next()) ]
        # Denormalization
        elif config[self.name]["experiment"] == constants.EXP_DENORMALIZATION:
            params = [ random.randint(0, self.num_articles) ]
        # Indexing
        elif config[self.name]["experiment"] == constants.EXP_INDEXING:
            params = [ random.randint(0, self.num_articles) ]
        else:
            raise Exception("Unexpected experiment type %d" % config["experiment"]) 
        
        return (txnName, params)
    ## DEF
        
    def executeImpl(self, config, txn, params):
        assert self.conn != None
        assert "experiment" in config
        
        if self.debug:
            LOG.debug("Executing %s / %s [denormalize=%s]" % (txn, str(params), config["denormalize"]))
        
        # Sharding Key
        if config[self.name]["experiment"] == constants.EXP_SHARDING:
            self.expSharding(params[0])
        # Denormalization
        elif config[self.name]["experiment"] == constants.EXP_DENORMALIZATION:
            self.expDenormalization(config["denormalize"], params[0])
        # Indexing
        elif config[self.name]["experiment"] == constants.EXP_INDEXING:
            self.expIndexes(params[0])
        # Busted!
        else:
            pass
        
        return
    ## DEF
    
    
    def expSharding(self, articleId):
        """
        For this experiment, we will shard articles by their autoinc id and then 
        by their id+timestamp. This will show that sharding on just the id won't
        work because of skew, but by adding the timestamp the documents are spread out
        more evenly. If we shard on the id+timestamp, will queries that only use the 
        timestamp get redirected to a mininal number of nodes?
        Not sure if this is a good experiment to show this. Might be too trivial.
        """
        article = self.db[constants.ARTICLE_COLL].find_one({"id": articleId}, {"comments": 0})
        if not article:
            LOG.warn("Failed to find %s with id #%d" % (constants.ARTICLE_COLL, articleId))
            pass
        return
    ## DEF
    
    def expDenormalization(self, denormalize, articleId):
        """
        In our microbenchmark we should have a collection of articles and collection of 
        article comments. The target workload will be to grab an article and grab the 
        top 10 comments for that article sorted by a user rating. In the first experiment,
        we will store the articles and comments in separate collections.
        In the second experiment, we'll embedded the comments inside of the articles.
        Not sure if we can do that in a single query... 
        What we should see is that the system is really fast when it can use a single 
        query for documents that contain a small number of embedded documents. But 
        then as the size of the comments list for each article increases, the two query
        approach is faster. We may want to also have queries that append to the comments
        list to show that it gets expensive to write to documents with a long list of 
        nested documents
        """
        
        article = self.db[constants.ARTICLE_COLL].find_one({"id": articleId})
        if not article:
            LOG.warn("Failed to find %s with id #%d" % (constants.ARTICLE_COLL, articleId))
            pass
        assert article["id"] == articleId
        if denormalize:
            comments = self.db[constants.COMMENT_COLL].find({"article": articleId})
        else:
            assert "comments" in article, pformat(article)
            comments = article["comments"]
        return
    ## DEF
    
    def expIndexes(self, articleId):
        """
        In our final benchmark, we compared the performance difference between a query on 
        a collection with (1) no index for the query's predicate, (2) an index with only one 
        key from the query's predicate, and (3) a covering index that has all of the keys 
        referenced by that query.
        What do we want to vary here on the x-axis? The number of documents in the collection?
        """
        
        article = self.db[constants.ARTICLE_COLL].find({"id": articleId}, {"id", "date", "author"})
        if not article:
            LOG.warn("Failed to find %s with id #%d" % (constants.ARTICLE_COLL, articleId))
        
        return
## CLASS